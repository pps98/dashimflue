import streamlit as st
import json
from datetime import datetime
import pandas as pd
from collections import Counter
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import plotly.express as px
import requests
from dotenv import load_dotenv
import os
from hashlib import md5
import diskcache
from tqdm import tqdm
import traceback  # Para exibir traceback completo em caso de erro

# Configura√ß√µes iniciais
load_dotenv(override=True)
API_KEY = os.getenv("API_KEY")

if not API_KEY:
    st.error("A chave API n√£o foi encontrada no arquivo .env.")
    st.stop()

# Configura√ß√£o do cache
cache = diskcache.Cache('./cache_normalizacao')

# Configura√ß√£o da p√°gina
st.set_page_config(page_title="Dashboard Comunidade", layout="wide")

# Fun√ß√£o para acessar a API do ChatGPT
def acessa_chatgpt(prompt):
    url = 'https://api.openai.com/v1/chat/completions'
    headers = {
        'Content-Type': 'application/json',
        'Authorization': f'Bearer {API_KEY}'
    }
    data = {
        'model': 'gpt-4o-mini',
        'messages': [{'role': 'user', 'content': prompt}],
        'temperature': 0.3
    }
    
    try:
        response = requests.post(url, headers=headers, json=data, timeout=60)
        response.raise_for_status()
        return response.json()['choices'][0]['message']['content']
    except Exception as e:
        st.error(f"Erro na API do ChatGPT: {str(e)}")
        st.error(traceback.format_exc())
        return None

# Fun√ß√µes auxiliares para processar dados
def extrair_mes(aniversario):
    try:
        aniversario_normalizado = aniversario.lower().replace('√£', 'a').replace('√µ', 'o')
        if 'informado' in aniversario_normalizado:
            return None
        partes = aniversario.split("/")
        if len(partes) >= 2:
            mes = int(partes[1])
            if 1 <= mes <= 12:
                return mes
    except Exception:
        pass
    return None

def extrair_estado(local):
    if pd.isna(local) or local.lower() in ["n√£o informado", "nao informado"]:
        return "N√£o informado"
    if '/' in local:
        return local.split('/')[-1].strip()
    if ',' in local:
        return local.split(',')[-1].strip()
    return local

def obter_interesses(perfil):
    interesses = perfil.get('interesses', [])
    if isinstance(interesses, list):
        return [tag.lower().strip() for tag in interesses if isinstance(tag, str) and tag.strip()]
    elif isinstance(interesses, str):
        return [tag.lower().strip() for tag in interesses.split('#') if tag.strip()]
    else:
        st.warning(f"Formato inesperado para 'interesses' no perfil: {perfil.get('nome', 'Sem Nome')}")
        return []

def obter_areas(perfil):
    area = perfil.get('√°rea', "")
    if isinstance(area, list):
        return [tag.lower().strip() for tag in area if isinstance(tag, str) and tag.strip()]
    elif isinstance(area, str):
        return [tag.lower().strip() for tag in area.split('#') if tag.strip()]
    else:
        st.warning(f"Formato inesperado para '√°rea' no perfil: {perfil.get('nome', 'Sem Nome')}")
        return []

def normalizar_dados(perfis):
    template = """Analise e padronize os seguintes campos para cada perfil:
- Local (formato: "Cidade/UF" ou "Cidade, Pa√≠s")
- √Årea (tags em lowercase sem caracteres especiais, separadas por #)
- Interesses (mesmo crit√©rio das √°reas)

Retorne APENAS um JSON com os perfis normalizados, com os campos:
- nome
- local
- √°rea
- interesses
- url linkedin
- newsletter
- aniversario

Perfis originais:
{perfis_json}
"""
    
    perfis_normalizados = []
    perfis_para_normalizar = []
    perfis_hashes = []

    for perfil in perfis:
        try:
            perfil_hash = md5(json.dumps(perfil, sort_keys=True).encode()).hexdigest()
            if perfil_hash in cache:
                perfis_normalizados.append(cache[perfil_hash])
            else:
                perfis_para_normalizar.append(perfil)
                perfis_hashes.append(perfil_hash)
        except Exception as e:
            st.error(f"Erro ao processar hash para perfil {perfil.get('nome', '')}: {str(e)}")
            st.error(traceback.format_exc())
            perfis_normalizados.append(perfil)

    if perfis_para_normalizar:
        batch_size = 20
        batches = [perfis_para_normalizar[i:i + batch_size] for i in range(0, len(perfis_para_normalizar), batch_size)]
        batch_hashes = [perfis_hashes[i:i + batch_size] for i in range(0, len(perfis_hashes), batch_size)]
        
        with st.spinner('Normalizando dados com IA...'):
            for batch, hashes in tqdm(zip(batches, batch_hashes), total=len(batches), desc="Processando perfis"):
                prompt = template.format(perfis_json=json.dumps(batch, ensure_ascii=False))
                resposta = acessa_chatgpt(prompt)
                if resposta:
                    try:
                        resposta_limpa = resposta.replace('```json', '').replace('```', '').strip()
                        perfis_batch_normalizados = json.loads(resposta_limpa)
                        
                        for perfil_normalizado, perfil_hash in zip(perfis_batch_normalizados, hashes):
                            if all(key in perfil_normalizado for key in ['nome', 'local', '√°rea', 'interesses']):
                                cache[perfil_hash] = perfil_normalizado
                                perfis_normalizados.append(perfil_normalizado)
                            else:
                                st.warning(f"Perfil {perfil_normalizado.get('nome', 'Sem Nome')} n√£o possui todos os campos necess√°rios. Mantendo o original.")
                                perfis_normalizados.append(perfil)
                    except json.JSONDecodeError as e:
                        st.error(f"Erro ao decodificar JSON: {str(e)}")
                        st.error(traceback.format_exc())
                        perfis_normalizados.extend(batch)
                else:
                    perfis_normalizados.extend(batch)

    return perfis_normalizados

def carregar_dados():
    url = "https://drive.google.com/file/d/1Rx5BRqQYIpvXfzX7wAAxPM7w1vF7F927/view?usp=drive_link"
    
    try:
        file_id = url.split('/d/')[1].split('/')[0]
        download_url = f"https://drive.google.com/uc?export=download&id={file_id}"
        response = requests.get(download_url)
        response.raise_for_status()
        return response.json()
    except Exception as e:
        st.error(f"Erro ao carregar arquivo: {str(e)}")
        st.error(traceback.format_exc())
        return None

def processar_tags(perfis, campo):
    tags = []
    for p in perfis:
        if campo == "interesses":
            tags.extend(obter_interesses(p))
        elif campo == "√°rea":
            tags.extend(obter_areas(p))
    return Counter(tags)

# Carregar e processar dados
if 'perfis' not in st.session_state:
    dados_brutos = carregar_dados()
    if dados_brutos:
        st.session_state.perfis = normalizar_dados(dados_brutos)
    else:
        st.stop()

# Sidebar - Filtros
st.sidebar.title("üîç Filtros")
local_options = ["Todos"] + sorted({p["local"] for p in st.session_state.perfis if p["local"] != "N√£o informado"})
local_selecionado = st.sidebar.selectbox("Localiza√ß√£o", local_options)

# Processamento de dados filtrados
dados_filtrados = [
    p for p in st.session_state.perfis 
    if local_selecionado == "Todos" or p["local"] == local_selecionado
]

# Converter dados_filtrados para DataFrame
df_dados_filtrados = pd.DataFrame(dados_filtrados)

# Opcional: Exibir os primeiros registros para verifica√ß√£o
# st.write("### Exemplos de Dados Normalizados", df_dados_filtrados.head())

# 1. Vis√£o Geral
st.title("üìä Dashboard (insights) - Imflue Circle")
col1, col2, col3, col4 = st.columns(4)
with col1:
    st.metric("Total de Membros", len(dados_filtrados))
with col2:
    st.metric("Localiza√ß√µes √önicas", len({p["local"] for p in dados_filtrados}))
with col3:
    newsletters_ativas = sum(1 for p in dados_filtrados if p.get("newsletter", "n√£o informado").lower() not in ["n√£o informado", "nao informado"])
    percentage = f"{(newsletters_ativas/len(dados_filtrados)*100):.1f}%" if len(dados_filtrados) > 0 else "0%"
    st.metric("Newsletters Ativas", f"{newsletters_ativas} ({percentage})")
with col4:
    mes_atual = datetime.now().month
    aniversariantes = []
    for p in dados_filtrados:
        aniversario = p.get("aniversario", "n√£o informado")
        if aniversario.lower() not in ["n√£o informado", "nao informado"]:
            mes = extrair_mes(aniversario)
            if mes == mes_atual:
                aniversariantes.append(p)
            elif mes is None:
                st.warning(f"Formato de anivers√°rio inv√°lido para o perfil: {p.get('nome', 'Sem Nome')}, anivers√°rio: '{aniversario}'")
    st.metric("Aniversariantes do M√™s", len(aniversariantes))

# 2. Distribui√ß√£o Geogr√°fica
st.subheader("üåç Distribui√ß√£o Geogr√°fica")

df_localizacao = pd.DataFrame([p["local"] for p in dados_filtrados], columns=["local"])
df_localizacao["estado"] = df_localizacao["local"].apply(extrair_estado)

estado_counts = (
    df_localizacao["estado"]
    .value_counts()
    .reset_index()
    .rename(columns={"index": "Estado", "estado": "Contagem"})
)

if not estado_counts.empty and {"Estado", "Contagem"}.issubset(estado_counts.columns):
    fig = px.bar(
        estado_counts,
        x="Estado",
        y="Contagem",
        labels={"Estado": "Localiza√ß√£o", "Contagem": "N√∫mero de Membros"},
        color="Estado"
    )
    st.plotly_chart(fig, use_container_width=True)
else:
    st.warning("Dados insuficientes para mostrar a distribui√ß√£o geogr√°fica")

# 3. √Åreas e Interesses
st.subheader("üîñ √Åreas e Interesses Principais")
col1, col2 = st.columns(2)

with col1:
    st.write("**Nuvem de Palavras**")
    todas_tags = processar_tags(dados_filtrados, "√°rea") + processar_tags(dados_filtrados, "interesses")
    if todas_tags:
        wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(todas_tags)
        plt.figure(figsize=(10,5))
        plt.imshow(wordcloud, interpolation='bilinear')
        plt.axis("off")
        st.pyplot(plt)
    else:
        st.write("Nenhuma tag dispon√≠vel para gerar a nuvem de palavras.")

with col2:
    st.write("**Top 10 √Åreas de Atua√ß√£o**")
    areas_top = processar_tags(dados_filtrados, "√°rea").most_common(10)
    if areas_top:
        df_areas_top = pd.DataFrame(areas_top, columns=["√Årea", "Contagem"])
        fig = px.bar(df_areas_top, x="√Årea", y="Contagem", labels={"√Årea": "√Årea", "Contagem": "N√∫mero de Membros"}, color="√Årea")
        st.plotly_chart(fig, use_container_width=True)
    else:
        st.write("Nenhuma √°rea dispon√≠vel para exibir.")

# 3.2 Treemap Hier√°rquico
st.write("**Distribui√ß√£o Hier√°rquica de √Åreas**")
try:
    data = []
    for p in dados_filtrados:
        areas = obter_areas(p)
        if len(areas) >= 2:
            data.append({"Categoria": areas[0].capitalize(), "Subcategoria": areas[1].capitalize()})
        elif len(areas) == 1:
            data.append({"Categoria": areas[0].capitalize(), "Subcategoria": "Geral"})
    
    df_hierarquia = pd.DataFrame(data)
    
    if not df_hierarquia.empty:
        fig = px.treemap(
            df_hierarquia,
            path=['Categoria', 'Subcategoria'],
            color='Categoria',
            color_continuous_scale='Rainbow'
        )
        st.plotly_chart(fig, use_container_width=True)
    else:
        st.write("Dados insuficientes para gerar o treemap.")
except Exception as e:
    st.error(f"Erro ao gerar treemap: {str(e)}")
    st.error(traceback.format_exc())

# 3.3 Mapa de Calor de Rela√ß√µes
st.subheader("üî• Rela√ß√£o entre √Åreas e Interesses")
try:
    # Selecionar as top 15 √°reas e interesses para evitar matrizes muito grandes
    top_areas = processar_tags(dados_filtrados, "√°rea").most_common(15)
    top_interesses = processar_tags(dados_filtrados, "interesses").most_common(15)
    areas = [area for area, _ in top_areas]
    interesses = [interesse for interesse, _ in top_interesses]
    
    # Criar DataFrame de rela√ß√µes
    data = []
    for p in dados_filtrados:
        p_areas = [tag.lower() for tag in obter_areas(p) if tag.lower() in areas]
        p_interesses = [tag.lower() for tag in obter_interesses(p) if tag.lower() in interesses]
        for area in p_areas:
            for interesse in p_interesses:
                data.append({"√Årea": area.capitalize(), "Interesse": interesse.capitalize()})
    
    df_relacoes = pd.DataFrame(data)
    if not df_relacoes.empty:
        pivot_table = df_relacoes.pivot_table(index='√Årea', columns='Interesse', aggfunc='size', fill_value=0)
        
        fig = px.imshow(
            pivot_table,
            labels=dict(x="Interesse", y="√Årea", color="Frequ√™ncia"),
            color_continuous_scale="YlOrRd",
            aspect="auto"
        )
        st.plotly_chart(fig, use_container_width=True)
    else:
        st.write("Dados insuficientes para gerar o mapa de calor.")
except Exception as e:
    st.error(f"Erro ao gerar mapa de calor: {str(e)}")
    st.error(traceback.format_exc())

# 4. Newsletters Ativas
st.subheader("üì¨ Newsletters Ativas")
newsletters = [p["newsletter"] for p in dados_filtrados if p.get("newsletter", "n√£o informado").lower() not in ["n√£o informado", "nao informado"]]
if newsletters:
    st.write("Links dispon√≠veis:")
    for link in newsletters:
        st.markdown(f"- [{link}]({link})")
else:
    st.write("Nenhuma newsletter informada para os filtros selecionados")

# 5. Aniversariantes
st.subheader("üéÇ Aniversariantes do M√™s")
if aniversariantes:
    for p in aniversariantes:
        st.markdown(f"**{p.get('nome', 'Sem Nome')}** - {p.get('aniversario', 'N√£o Informado')}")
else:
    st.write("Nenhum aniversariante neste m√™s")

# 5.1 Calend√°rio de Anivers√°rios
st.subheader("üìÖ Distribui√ß√£o de Anivers√°rios no Ano")
try:
    df_aniversarios = pd.DataFrame([p.get("aniversario", "") for p in dados_filtrados if p.get("aniversario", "n√£o informado").lower() not in ["n√£o informado", "nao informado"]], columns=["Data"])
    # Remover entradas inv√°lidas
    df_aniversarios = df_aniversarios[df_aniversarios["Data"].str.contains("/")]
    df_aniversarios[['Dia', 'M√™s']] = df_aniversarios['Data'].str.split('/', expand=True).astype(float, errors='ignore')
    df_aniversarios = df_aniversarios.dropna()
    
    if not df_aniversarios.empty:
        fig = px.density_heatmap(
            df_aniversarios,
            x="M√™s",
            y="Dia",
            nbinsx=12,
            nbinsy=31,
            color_continuous_scale="Viridis",
            title="Frequ√™ncia de Anivers√°rios por Dia/M√™s"
        )
        st.plotly_chart(fig, use_container_width=True)
    else:
        st.write("Dados insuficientes para gerar o calend√°rio de anivers√°rios.")
except Exception as e:
    st.error(f"Erro ao gerar calend√°rio: {str(e)}")
    st.error(traceback.format_exc())

# 6. Busca de Perfis
st.subheader("üîç Busca de Membros")
busca = st.text_input("Pesquisar por nome, √°rea ou palavra-chave:")
if busca:
    busca_lower = busca.lower()
    resultados = [
        p for p in dados_filtrados 
        if busca_lower in p.get('nome', '').lower() 
        or busca_lower in ' '.join(obter_areas(p)).lower() 
        or busca_lower in ' '.join(obter_interesses(p)).lower()
    ]
else:
    resultados = []

if resultados:
    for perfil in resultados:
        with st.expander(f"{perfil.get('nome', 'Sem Nome')} - {perfil.get('local', 'N√£o Informado')}"):
            st.write(f"**√Årea:** {' # '.join(obter_areas(perfil))}")
            st.write(f"**Interesses:** {' # '.join(obter_interesses(perfil))}")
            if perfil.get('url linkedin', None):
                st.markdown(f"**[LinkedIn]({perfil['url linkedin']})**")
else:
    st.write("Nenhum resultado encontrado")

# 6.1 Gr√°fico de Dispers√£o de Atividades com Interesses Detalhados
st.subheader("üéØ Distribui√ß√£o de Atividades por Localiza√ß√£o")

try:
    # Verificar se 'local', '√°rea' e 'interesses' est√£o presentes
    required_columns = ['local', '√°rea', 'interesses']
    if not all(col in df_dados_filtrados.columns for col in required_columns):
        st.error("Dados insuficientes para gerar o gr√°fico de dispers√£o.")
    else:
        # Agrupar dados por Localiza√ß√£o com nomes de colunas √∫nicos
        agrupado = df_dados_filtrados.groupby('local').agg(
            Numero_Areas=pd.NamedAgg(column='√°rea', aggfunc=lambda x: sum(len(obter_areas({'√°rea': a})) for a in x)),
            Numero_Interesses=pd.NamedAgg(column='interesses', aggfunc=lambda x: sum(len(obter_interesses({'interesses': i})) for i in x)),
            Interesses=pd.NamedAgg(column='interesses', aggfunc=lambda x: Counter([interest for p in x for interest in obter_interesses({'interesses': p})]))
        ).reset_index()
        
        # Obter top 3 interesses por localiza√ß√£o
        agrupado['Top_Interesses'] = agrupado['Interesses'].apply(lambda c: ', '.join([interest for interest, count in c.most_common(3)]))
        
        # Renomear colunas para nomes √∫nicos
        df_atividades = agrupado.rename(columns={
            'local': 'Local',
            'Numero_Areas': '√Åreas',
            'Numero_Interesses': 'Total_Interesses'  # Renomear para evitar duplica√ß√£o
        })
        
        # Depura√ß√£o: Exibir os dados agrupados
        st.write("### Dados Agrupados para o Gr√°fico de Dispers√£o", df_atividades.head())
        
        # Criar gr√°fico de dispers√£o com hover info detalhado
        fig = px.scatter(
            df_atividades,
            x="√Åreas",
            y="Total_Interesses",  # Atualizar para o novo nome
            color="Local",
            size="√Åreas",
            hover_data={'Top_Interesses': True, 'Local': True, '√Åreas': True, 'Total_Interesses': True},
            title="Rela√ß√£o entre N√∫mero de √Åreas e Interesses por Localiza√ß√£o",
            labels={
                "Local": "Localiza√ß√£o",
                "√Åreas": "N√∫mero de √Åreas",
                "Total_Interesses": "N√∫mero de Interesses"
            }
        )
        
        # Atualizar o template de hover para incluir os top interesses
        fig.update_traces(
            hovertemplate=
            '<b>%{customdata[0]}</b><br>' +
            '√Åreas: %{x}<br>' +
            'Interesses: %{y}<br>' +
            'Top Interesses: %{customdata[1]}<extra></extra>'
        )
        
        st.plotly_chart(fig, use_container_width=True)
except Exception as e:
    st.error(f"Erro ao gerar gr√°fico de dispers√£o: {str(e)}")
    st.error(traceback.format_exc())

# 7. Insights de Networking
st.subheader("ü§ù Sugest√µes de Conex√£o")
if len(dados_filtrados) >= 2:
    st.write("Membros com interesses complementares:")
    pares = []
    for i in range(len(dados_filtrados)):
        interesses1 = set(obter_interesses(dados_filtrados[i]))
        for j in range(i+1, len(dados_filtrados)):
            interesses2 = set(obter_interesses(dados_filtrados[j]))
            if interesses1 & interesses2:
                pares.append((dados_filtrados[i].get('nome', 'Sem Nome'), dados_filtrados[j].get('nome', 'Sem Nome')))
                if len(pares) >= 3:
                    break
        if len(pares) >= 3:
            break
    if pares:
        for par in pares:
            st.write(f"- {par[0]} ‚Üî {par[1]}")
    else:
        st.write("Nenhum par encontrado com interesses complementares.")
else:
    st.write("Selecione uma localiza√ß√£o com mais membros para sugest√µes")

# Rodar com: streamlit run dashboard.py
